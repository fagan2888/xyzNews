\documentclass{beamer}

\usetheme{Boadilla}
\usepackage{natbib}
\usepackage{hyperref}

\title{Variation in Political News}
\subtitle{An NLP Approach}
\author{Stephen Lee}
\institute{University of Memphis}

\begin{document}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TITLE SLIDES

	\begin{frame}
		\maketitle
	\end{frame}

    \begin{frame}
    	\frametitle{Outline}
    	\tableofcontents
    \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRO

\section{Introduction}

    \subsection{Background}
   
    \begin{frame}
   	    \frametitle{Background}
   	    \begin{center}
   	    	\textit{``If you can't measure it, you can't improve it.''}
   	    \end{center}
   	    \begin{flushright}
   	    	--- Lord Kelvin\footnote{The actual source of this quote is hard to pin down. While some attribute the quote to Lord Kelvin, others attribute it to Peter Drucker. A comment on stackoverflow further suggests that Antoine-Augustin Cournot was actually the first to express it in concise form in ``De l'origine et des limites de la correspondance entre l'algebre et la geometrie'' in 1847.}
   	    \end{flushright}
       
       \begin{itemize}
       	\item The 2016 United States Presidential Election raised doubts for many about the quality of their news.
       	\item \citet{allcott2017social} estimate that the average US adult read and remembered about one, and possibly up to several, fake news articles during the election period.
       	\item While the ``fake news'' problem is often referenced, a full solution may not actually exist. 
       \end{itemize}
    \end{frame}
   
	\subsection{Literature Review}
	
	\begin{frame}
		\frametitle{Literature Review: Computer Science}
		\begin{itemize}
			\item \citet{volkova2017separating}, \citet{wang2017liar}, and \citet{shu2017fake} all look at detecting real vs. fake news. 
			\item This isn't the same question, however. We need tools. 
			\item The LSTM architecture was introduced in \citet{hochreiter1997long}. \citet{greff2016lstm} show several variations to all be roughly equivalent. 
			\item \citet{schuster1997bidirectional} introduced the first bidirectional RNN. 
			\item Together, bidirectional LSTM architectures prove to be among the most accurate models for language tasks, consistent with \citet{wang2015unified}. 
		\end{itemize}
	\end{frame}
	
	\begin{frame}
		\frametitle{Literature Review: Economics}
		\begin{itemize}
			\item \citet{gentzkow2010drives} find that readers prefer to consume ``like-minded'' news and this can account for around 20\% of the variation in political slant or bias.
			\item \citet{gentzkow2006media} also show that a Bayesian consumer will reinforce their beliefs of a given news source quality when they read something that confirms their priors.
			\item \citet{gentzkow2008competition} suggest that competition in information markets may actually be counterproductive. 
			\item Together, these works suggest that political news bias may tactical, and that this polarization we see may be self-reinforcing.
		\end{itemize}
	\end{frame}

    \subsection{Summary}
    
    \begin{frame}
    	\frametitle{Summary}
    	\begin{itemize}
    		\item With a goal of classifying articles by their news source, I scraped several thousand news articles from Fox, Vox, and PBS News.
    		\item I found that a bidirectional, LSTM recurrent neural network can correctly predict the source of an article with high accuracy. 
    		\item I then used the result of this trained network to build a web app that can allow for a copy-and-paste interface to interact with this classification model. 
    		\item To my knowledge, this work provides the first thorough NLP based approach to understanding semantic differences across news sources. 
    	\end{itemize}
    \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DATA 

\section{Data}

    \begin{frame}
    	\frametitle{Data}
    	\begin{itemize}
    		\item I mined political news articles from the websites of Fox News, Vox News, and PBS News. 
    		\item Intuitively, the motivation is to focus only on sentiment or semantics, rather than subject matter differences.
    		\item By some estimations,\footnote{Source: https://www.adfontesmedia.com.} these three news sites represent distinct categories of news: 
    		\begin{itemize}
    			\item Fox as a conservative “right” opinion. 
    			\item PBS as the “center” primary source news position. 
    			\item Vox as a liberal “left” opinion.
    		\end{itemize} 
    	\end{itemize}
    \end{frame}

    \begin{frame}
    	\begin{figure}[H]
    		\includegraphics[width=\textwidth]{figures/images/news-bias.jpg}
    		\caption{A graphical depiction of the bias in various political news sources.}
    		\label{fig:news-bias}
    	\end{figure}
    \end{frame}

	\begin{frame}
		\frametitle{Descriptive Statistics}
		\input{figures/summary_table.tex}
	\end{frame}

	\begin{frame}
		\frametitle{N-gram Frequencies}
		\input{figures/word_frequencies.tex}
	\end{frame}

	\begin{frame}
		\frametitle{N-gram Frequencies}
		\input{figures/2gram_frequencies.tex}
	\end{frame}

    \subsection{Challenges}
    
    \begin{frame}
    	\frametitle{Challenges}
		\begin{enumerate}
			\item Difference in corpus size from each source. 
			\begin{itemize}
				\item Bootstrap the data. 
			\end{itemize}  
		
		    \item Variability of online formatting. 
		    \begin{itemize}
		    	\item I removed any mention of their own organization.
		    	\item Other common and unique affiliations. 
		    	\item Any other noticeable identifying characteristics.
		    \end{itemize}   
	    
	        \item Difference in the average article length. 
	        \begin{itemize}
	        	\item I limited the article length to the first 500 words.
	        \end{itemize}
		\end{enumerate}
    \end{frame}
    
	\subsection{Statistical Analysis}
	
	\begin{frame}
		\frametitle{Statistical Analysis}
		\begin{itemize}
			\item Based on \citet{taddy2013multinomial}, I perform a statistical analysis of the data to see what words are most related to the various sources.
			\item Intuitively, consider speech as making draws from a multinomial distribution, based on your underlying ``sentiment''. 
			\item Using the multinomial inverse regression, we can compare to see what phrases are most associated with which news source. 
		\end{itemize}
	\end{frame}

	\begin{frame}
		\frametitle{Vox and PBS}
		\input{figures/vox_pbs_contributions.tex}
	\end{frame}

	\begin{frame}
		\frametitle{PBS and Fox}
		\input{figures/pbs_fox_contributions.tex}
	\end{frame}

	\begin{frame}
		\frametitle{Vox and Fox}
		\input{figures/vox_fox_contributions.tex}
	\end{frame}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NEURAL NETWORK MODEL

\section{LSTM Model}

     \begin{frame}
	   	\frametitle{Word Embedding}
	   	\begin{itemize}
	   		\item I use the common crawl 840B Global Word Vector (i.e. GloVe).
	   		\item Introduced in \citet{pennington2014glove} and uses 840 billion tokens and a case-sensitive vocabulary of 2.2 million words to map words into a corresponding $300 \times 1$ dimensional vector. 
	   		\item Accordingly, I do minimal preprocessing to the text besides the basic cleaning mentioned previously. 
	   	\end{itemize}
	   \end{frame}

    \begin{frame}
    	\frametitle{LSTM Model}
    	\begin{figure}[H]
    		\includegraphics[width=\textwidth]{figures/images/lstm-math.png}
    		\caption{A graphical depiction of a single LSTM cell.\footnote{Source: https://colah.github.io/posts/2015-08-Understanding-LSTMs}}
    	\end{figure}
    \end{frame}

    \begin{frame}
    	\frametitle{Bidirectional Training}
    	\begin{figure}[H]
    		\includegraphics[width=\textwidth]{figures/images/bidirectional-net.png}
    		\caption{A visual representation of a bidirectional LSTM training.}
    	\end{figure}
    \end{frame}

    \begin{frame}
    	\frametitle{Methodology}
    	\begin{itemize}
    		\item I split the data into training (90\%) and testing (10\%). 
    		\item Training data: fit a birdirectional LSTM using a range of parameterizations. 
    		\item Testing data: make predictions, compare to actual, and calculate F1 scores.
    		\item Similarly, I compare the bidirectional model to a forward only model using the same approach. 
    	\end{itemize}
        Note, 
        \begin{equation*}
        F_1 =  2 \,\, \frac{Precision \cdot Recall}{Precision + Recall}
        \end{equation*}
        Where, 
        \begin{align*}
        Precision &= \frac{True Positives}{True Positives + False Positives}\\ \\
        Recall &= \frac{True Positives}{True Positives + False Negatives}
        \end{align*}
    \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% RESULTS

\section{Results}

    \begin{frame}
    	\frametitle{Results: Bidirectional LSTM}
    	\input{figures/training_results.tex}
    \end{frame}

    \begin{frame}
    	\frametitle{Results: Forward LSTM}
		\input{figures/regular_training_results.tex}
    \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% IMPLEMENTATION 

\section{Implementation}

     \begin{frame}
    	\frametitle{Demo}
    	\begin{figure}[H]
    		\includegraphics[width=\textwidth]{figures/images/web-form.png}
    		\caption{The web app's home page. \href{http://35.245.203.56:5000}{Link to page.}}
    		\label{fig:form}
    	\end{figure}
    \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CONCLUSION 

\section{Conclusion and Discussion}

     \begin{frame}
    	\frametitle{Conclusion and Discussion}
    	\begin{itemize}
    		\item I've shown that NLP techniques can accurately classify news articles based on language differences in the underlying news sources. 
    		\item Given \citet{gentzkow2008competition} and \citet{gentzkow2006media}, it seems unlikely that biased news (or even fake news) will disappear anytime soon. 
    		\item A web application based on these ideas could serve as a starting point to measure bias in our news consumption. Analogous to: 
    		\begin{itemize}
    			\item Calendars can help to measure our time use.
    			\item Nutrition apps measure our macronutrients. 
    			\item GPS measures our geographical position.
    			\item Can we have an app to measure our news consumption? 
    		\end{itemize} 
    	\end{itemize}
    \end{frame}

    \begin{frame}
    	\frametitle{End}
    	\centering \Huge Thank you for your time.
    	
    	\centering Questions?
    \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% REFERENCES

\section{References}
	\begingroup
	\tiny
    \begin{frame}
    	\frametitle{References}
    	\bibliographystyle{plainnat}
    	\bibliography{references/ref}
    \end{frame}
    \endgroup


\end{document}