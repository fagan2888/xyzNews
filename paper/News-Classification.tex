\documentclass{article}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{lscape}

\author{Stephen M. Lee}
\title{A NLP Approach to Understanding Variation in Political News}

\begin{document}
	\maketitle 
	
	\newpage
	
	\tableofcontents
	
	\newpage
	
	\section{Introduction}
		The 2016 United States Presidential Election, to many, raised questions as to the reliability of their news sources [SOURCE NEEDED]. Since then, many social media companies and other institutions have begun public campaigns to combat the perceived threat from fake news, with arguably limited results [SOURCE NEEDED]. With a goal of news source classification, I scraped several thousand news articles from Fox, Vox, and PBS News. By some estimations [SOURCE NEEDED], these three represent distinct categories of news: Fox is often considered extreme “right” opinion (i.e. conservative); Vox is considered extreme “left” opinion (i.e. liberal); and PBS is considered “center” primary source news. I first calculate the top word, 2-gram, and 3-gram frequencies to better understand the dataset, and then train a Bidirectional LSTM neural network with pretrained embeddings to classify the news source. 
		\subsection{Literature Review}
	
	\section{Data}
	    I mined political news articles from the websites of Fox News, Vox News, and PBS News. Importantly, I restricted focus to only URLs that contained an explicit reference to politics i.e. from their respective political sections. This allowed me to collect articles that were as similar to each other as possible to try and limit the chances of spurious predictive results. Intuitively, the motivation behind this is to facilitate classifiacation based only on sentiment or semantics, rather than subject matter differences. A summary table of my data is shown in Table \ref{tab:summary}.  
 
    	\input{figures/summary_table.tex}
	    
	    This table shows that, from a high level view, the articles are semantically similar, on average. Specifically, I look at the average percent of adjectives and adverbs (including comparatives and superlatives) in a given document as a proxy for description. Further, I calculate the average number of words per sentence to proxy for complexity. 
	    
	    To better understand how similar the content of these articles are, we can construct n-gram tokens and count the frequency of their occurrences. In Table \ref{tab:ngram}, we see the most frequent one, two, and three-gram phrases. 
	    
	    \input{figures/ngram_freq.tex}
	    
	\subsection{Challenges}
	    There are several limitations to discuss. The most obvious is the difference in corpus size from each source. In particular, Fox News has fewer documents than either PBS or Vox by quite a large number. Fortunately, however, there are many well established best practices for dealing with imbalanced data [SOURCES]. In this work, I bootstrap the data for balance. Second, due to the variability of online formatting, it's worth noting the possibility that, even after cleaning, each source exhibits some subtle idiosyncratic standards that could allow a neural network to detect those instead of pure sentiment and semantic differences. To mitigate this, I removed any mention of their own organization, or any other unique affiliations or locations.\footnote{For example, for every Fox News article, I removed any mention of `Fox'. Additionally, Fox News cited the `Associated Press' disproportionately often, so I also removed that string.} Finally, each news source shows a significant difference in the average length of each article. To overcome this, I limited the article input size to the first 500 words to ensure that no single source was consistently shorter. 
	    	
	\section{Models}
	
	    
	
	\section{Results}
	
	\section{Implementation}
	
	\section{Conclusion and Discussion}
	
\end{document}